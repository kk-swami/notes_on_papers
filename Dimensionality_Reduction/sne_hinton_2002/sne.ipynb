{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Stochastic Neighbour embedding](https://papers.nips.cc/paper/2276-stochastic-neighbor-embedding)\n",
    "\n",
    "\n",
    "Briefly, SNE is described below\n",
    "\n",
    "1) Goal  - To convert m points/samples in high dimensional space x<sup>1</sup>, x<sup>2</sup>.. x<sup>m</sup>  to a lower dimensional space y<sup>1</sup>, y<sup>2</sup>.. y<sup>m</sup> such that they can be better visualized\n",
    "\n",
    "2) Uses pairwise similarity - A pair of points x<sup>i</sup> and x<sup>j</sup> are mapped to a pair of points y<sup>i</sup> and y<sup>j</sup> such that the <i><b>similarity</b></i> between the points in the high and low dimension is preserved\n",
    "\n",
    "3) How is <i><b>similarity</b></i> defined ?\n",
    "The similarity of two points in high dimensional space x<sup>i</sup> and x<sup>j</sup> is the <b>conditional probability</b> p<sub>j|i</sub> that x<sup>i</sup> would pick x<sup>j</sup> as a neighbour; if neighbours were picked under a gaussian distribution centered on x<sup>i</sup>\n",
    "Mathematically,\n",
    "\n",
    "p<sub>j|i</sub> = $\\frac{e^{-\\frac{|x^{i}-x^{j}|^{2}}{2\\sigma_i^{2}}}}{\\sum_{k,k!=i}{e^{-\\frac{|x^{i}-x^{k}|^{2}}{2\\sigma_i^{2}}}}}$\n",
    "\n",
    "4) Similarly , the similarity of points in low dimension space is defined as\n",
    "\n",
    "q<sub>j|i</sub> = $\\frac{e^{-|y^{i}-y^{j}|^{2}}}{\\sum_{k,k!=i}{e^{-{|y^{i}-y^{k}|^{2}}}}}$\n",
    "\n",
    "  (Note that in low dimensional space, $\\sigma_i$ does not exist)\n",
    "\n",
    "5) We define cost function which we want to minimize as the sum of  KL divergence between $P_i$ and $Q_i$\n",
    "\n",
    "    C = $\\sum_iKL(P_i/Q_i)$\n",
    "      = $\\sum_i\\sum_jp_{j|i}*ln\\frac{p_{j|i}}{q_{i|i}}$\n",
    "\n",
    "\n",
    "  Therefore, goal of optimization to arrive at y<sup>i</sup>, i=1..m in such a way that this cost function is minimized\n",
    "\n",
    "\n",
    "6) Note that the above cost is not symmetric. In particular, there is a large cost if $q_{j|i}$ is small and $p_{j|i}$ is large than the other way round -> if points are close together in original space and far apart in low dimensional space, they are penalized; wherease only small cost if points are far in original space but close in lower dimensional space, not penalized so much\n",
    "\n",
    "6) How is $\\sigma_i$ chosen ? Given perplexity (which is equivalent to no of effective neighbours for every point) as an input to the SNE algorithm, we can choose $\\sigma_i$ for every point x<sup>i</sup> in such a way that same perplexity is maintained for all sites (choose less no of points if x<sup>i</sup> has more neighbours and more if x<sup>i</sup> has less)\n",
    "\n",
    "This is implemented algorithmically by using a binary search to find right value of $\\sigma_i$ for every i. $Perplexity_i = 2^{H_i}$ where H is the Shannon entropy ; $H_i = -\\sum_{j}P_{j|i}ln(P_{j|i})$\n",
    "\n",
    "7) Minimize C by gradient descent.   $y^{i}_{t+1} = y^{i}_{t} - learningrate*\\frac{\\partial C}{\\partial y^i}$ for all i . (Of course, nowadays, you can choose computational graph formats like pytorch :) )\n",
    "\n",
    "If you choose to use gradient descent, the gradient has a very simple form:\n",
    "$\\frac{\\partial C}{\\partial y_i} = 2*\\sum_j(p_{j|i}-q_{j|i} + p_{i|j} - q_{i|j})(y_i - y_j)$\n",
    "\n",
    "(Derivation of this - TBD)\n",
    "(indices have been changed from i,j to k,l for easier derivative computation - handling indices while taking derivative wrt $y_i$)\n",
    "\n",
    "C = $\\sum_k\\sum_lp_{l|k}*ln\\frac{p_{l|k}}{q_{l|k}}$\n",
    "  = $\\sum_k\\sum_lp_{l|k}*lnp_{l|k} - \\sum_k\\sum_lp_{l|k}*ln{q_{l|k}}$\n",
    "\n",
    "Let C1 = $\\sum_k\\sum_lp_{l|k}*lnp_{l|k}$.\n",
    "This is a term independent of $y_i$ (no $q$ term), does not matter for derivative\n",
    "\n",
    "From definition, q<sub>l|k</sub> = $\\frac{e^{-|y^{k}-y^{l}|^{2}}}{\\sum_{t,t!=k}{e^{-{|y^{k}-y^{t}|^{2}}}}}$\n",
    "\n",
    "Let q<sub>l|k</sub> = $\\frac{D_{kl}}{Z_k}$ where\n",
    "$D_{kl} = e^{-|y^{k}-y^{l}|^{2}}$, $Z_k = \\sum_{t,t!=k}{e^{-{|y^{k}-y^{t}|^{2}}}}$\n",
    "\n",
    "$\\sum_k\\sum_lp_{l|k}*ln{q_{l|k}}$ =\n",
    "\n",
    "$\\sum_k\\sum_kp_{l|k}*ln{D_{kl}}$ - $\\sum_k\\sum_lp_{l|k}*ln{Z_k}$\n",
    "\n",
    "Therefore, C = C1 - $\\sum_k\\sum_lp_{l|k}*ln{D_{kl}}$ + $\\sum_k\\sum_lp_{l|k}*ln{Z_k}$\n",
    "\n",
    "\n",
    "C = C1 - $\\sum_k\\sum_lp_{l|k}*ln{D_{kl}}$ +\n",
    "$\\sum_kln{Z_k}\\sum_lp_{l|k}$   (as $Z_k$ is independent of l)\n",
    "\n",
    "Note that $\\sum_lp_{l|k}$ = 1 for all k\n",
    "\n",
    "\n",
    "Therefore, C = C1 - $\\sum_k\\sum_lp_{l|k}*ln{D_{kl}}$   + $\\sum_kln{Z_k}$\n",
    "\n",
    "\n",
    "**Partial Derivative of Term 1 wrt $y_i$ - $\\frac {\\partial C_1}{y_i}$**\n",
    "\n",
    "$\\frac {\\partial C_1}{y_i}$ = 0 as C1 is independent of i\n",
    "\n",
    "\n",
    "**Partial derivative of term 2 wrt y_i**\n",
    "\n",
    "In, $\\sum_k\\sum_lp_{l|k}*ln{D_{kl}}$ the only terms which depend on i are\n",
    "$\\sum_kp_{i|k}lnD_{ik}  + \\sum_lp_{l|i}lnD_{il}$, which replacing k and l by j is equivalent to\n",
    "\n",
    "$\\sum_j (p_{i|j}lnD_{ji}  + p_{j|i}lnD_{ij})$\n",
    "\n",
    "Note that $\\frac{\\partial D_{ij}}{\\partial y_i}$ = $D_{ij}*-2*(y_i - y_j)$\n",
    "\n",
    "Therefore,\n",
    "$\\frac{\\partial term2}{\\partial y_i}$ = $\\frac{\\partial \\sum_j (p_{i|j}lnD_{ji}  + p_{j|i}lnD_{ij})}{\\partial y_i}$\n",
    "\n",
    "= $\\sum_j(p_{i|j}*-2*(y_i - y_j) + p_{j|i}*-2*(y_j - y_i))$\n",
    "\n",
    "\n",
    "**Partial derivative of term 3 wrt y_i** - changing k to j for index notation dor similarity with step 2 derivative\n",
    "$\\frac{\\partial \\sum_jln{Z_j}}{\\partial y_i}$\n",
    "\n",
    "= $\\sum_k\\frac{1}{Z_k}*\\frac{\\partial Z_k}{\\partial y_i}$\n",
    "\n",
    "Note that $\\frac{\\partial Z_k}{\\partial y_i}$ =\n",
    "\n",
    "\n",
    "\n",
    "$\\frac{\\partial \\sum_{t,t!=k}{e^{-{|y^{k}-y^{t}|^{2}}}}}{\\partial y_i}$\n",
    "\n",
    "The only terms here which depend on $y_i$ are 1) when k = i  i.e.  ${\\frac{1}{Z_i}\\frac{\\partial Z_i}{\\partial y_i}}$}$ for all t and 2) when k is iterated over all values except i,\n",
    "\n",
    "$\\sum_{k!=i}e^{-|y_k-y_i|^2}$ for all k\n",
    "\n",
    "\n",
    "Therefore,  $\\frac{\\partial Z_k}{\\partial y_i}$   reduces to (replacing t and k by j for convenience in 1) and 2) )\n",
    "\n",
    "\n",
    "\n",
    "$\\sum_k\\frac{1}{Z_k}*\\frac{\\partial Z_k}{\\partial y_i}$\n",
    "\n",
    "= $\\sum_i\\frac{1}{Z_k}*\\frac{\\partial Z_k}{\\partial y_i}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\frac {\\partial \\sum_kln{Z_k} }{\\partial y_i}$ = $\\frac {\\partial ln{Z_i} }{\\partial y_i}$  (as all other terms are independent of y_i)\n",
    "\n",
    "= $\\frac{1}{Z_i}\\frac{\\partial Z_i}{\\partial y_i}$\n",
    "\n",
    "$\\frac{\\partial Z_i}{\\partial y_i}$ = $\\frac{\\partial \\sum_{t,t!=i}{e^{-{|y^{i}-y^{t}|^{2}}}}}{\\partial y_i}$\n",
    "\n",
    "= $\\sum_{t,t!=i}\\frac{\\partial e^{-|y^{i}-y^{t}|^{2}}}{\\partial y_i}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial e^{-|y^{i}-y^{t}|^{2}}}{\\partial y_i}$ = $e^{-|y^{i}-y^{t}|^{2}}$*${-2}*(y^{i}-y^{t})$\n",
    "\n",
    "=>\n",
    "\n",
    "$\\frac{\\partial Z_i}{\\partial y_i}$ = $\\sum_{t,t!=i}e^{-|y^{i}-y^{t}|^{2}}$*${-2}*(y^{i}-y^{t})$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial y_i}$ =\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8) Practical details of implementation\n",
    "  i)  A large momentum term is added to increase convergence rate\n",
    "  ii) In early stages of optimization, gaussian noise is added to the map points after each iteration. This loss is slowly reduced with time -  a la simulated annealing - Algorithm sensitive to initial amount of gaussian noise and rate at which it decays. Run algorithm several times with different values of hyper-parameters (momentum parameters, learning rate,  and amount of momentum) - Therefore, no convex optimization, inferior to methods which allow convex optimization\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
