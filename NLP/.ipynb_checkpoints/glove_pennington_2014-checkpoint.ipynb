{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation  \n",
    "\n",
    "GloVe: Global Vectors for Word Representation\n",
    "Pennington, Socher, Manning 2014\n",
    "\n",
    "# Tags  \n",
    "\n",
    "vector space models, representations, embeddings\n",
    "\n",
    "# Significance\n",
    "\n",
    "A new technique to get vector representations of words/embeddings which produces a word-vector space with meaningful sub-structure \n",
    "i.e. does well on word analogy tasks such as king-queen = man-woman , + leverage statistical information which models like skipgram w2vec do not do\n",
    "State of the art results on word analogy data sets, + downstream tasks like NER\n",
    "\n",
    "\n",
    "# Context and summary  \n",
    "\n",
    "Previously Existing Techiques :  \n",
    "Two main classes of techniques - \n",
    "\n",
    "\n",
    "## Global matrix factorization techniques like LSA and other techniques  \n",
    "1) Adv and Disadv : Leverage distributional information in corpus, as they primarily work off normalized word counts. These word well for distance based metrics, but not so well for word analogy tasks (king - queen = main - woman)  \n",
    "2) Start with Term document matrix or word word matrix of event counts, do transformations , dimensionality reduction to avoid disproportionate impact of irrelevant information like cooccurence counts of the with other words . Use PMI, other techniques to do so  \n",
    "\n",
    "    \n",
    "\n",
    "## Local context window methods, such as Mikolov's skip gram techniques which do well on analogy tasks, but do not leverage distributional information in corpus  \n",
    "1) Bengio (2003) uses a simple neural architecture,  Mikolov (2013a) uses skip gram and CBOW (single layer neural architectures), Levy (2014) - explicit word embeddingds based on PPMI metric  , vector log bilinear techniques (Mnih and Kavukcuoglo 2013) like vLBL and ivLBL)  \n",
    "2) CBOW and vLBL predict word given context, and skip gram and ivLBL predict context given word  \n",
    "3) These methods do not use cooccurence information , thus failing to take advantage of repetion in data (?)   \n",
    "\n",
    "\n",
    "# Glove  \n",
    "\n",
    "1) Let X be word word matrix of co-occurence counts, X<sub>ij</sub> be count of word j in context i, X<sub>i</sub> be no of times any word appears in context of word i = $\\sum_k{X_{ik}}$ (rowsum of counts). Let $p_{j|i} = X_{ij}/X_{i}$ be the conditional probability of word j appearing in context i   \n",
    "\n",
    "\n",
    "2) Instead of conditional probabilities themselves, the *ratio* of conditional probabilities in the presence of a third word k is better able to distinguish relevant and irrelevant words, and also discriminate betwee two relevant words.   \n",
    "\n",
    "\n",
    "3) In figure pasted below (figure 1 of paper) Suppose we want to get representations of word i = ice, and j = steam.  \n",
    "a) For word k related to ice, but not to steam, $p_{k|ice}/p_{k|steam}$ will be very high  \n",
    "b) for word k related to steam but not ice, $p_{k|ice}/p_{k|steam}$ will be very small  \n",
    "c) and for word k which is equally related or unrelated to ice and steam, $p_{k|ice}/p_{k|steam}$ ~ 1\n",
    "\n",
    "\n",
    "![how_ratio_of_prob_is_better_than_raw_prob](glove_pic1.png \"Image Credit Figure 1 in paper\")       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4) This suggests that we want to learn word vector representations w<sub>i</sub>,  w<sub>j</sub> and  $w_{\\tilde{k}}$ (which is a context word vector); in such a way that\n",
    "\n",
    "$F(w_{i},w_{j},w_{\\tilde{k}}) = \\frac{p_{k|i}}{p_{k|j}}$   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "asa = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
