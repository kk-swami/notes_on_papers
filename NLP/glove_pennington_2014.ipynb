{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags  \n",
    "\n",
    "vector space models, representations, embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation  \n",
    "\n",
    "GloVe: Global Vectors for Word Representation\n",
    "Pennington, Socher, Manning 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance\n",
    "\n",
    "A new technique to get vector representations of words/embeddings which produces a word-vector space with meaningful sub-structure \n",
    "i.e. does well on word analogy tasks such as king-queen = man-woman , + leverage statistical information which models like skipgram w2vec do not do\n",
    "State of the art results on word analogy data sets, + downstream tasks like NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context and summary  \n",
    "\n",
    "Previously Existing Techiques :  \n",
    "Two main classes of techniques - \n",
    "\n",
    "\n",
    "## Global matrix factorization techniques like LSA and other techniques  \n",
    "1) Adv and Disadv : Leverage distributional information in corpus, as they primarily work off normalized word counts. These word well for distance based metrics, but not so well for word analogy tasks (king - queen = main - woman)  \n",
    "2) Start with Term document matrix or word word matrix of event counts, do transformations , dimensionality reduction to avoid disproportionate impact of irrelevant information like cooccurence counts of the with other words . Use PMI, other techniques to do so  \n",
    "\n",
    "    \n",
    "\n",
    "## Local context window methods, such as Mikolov's skip gram techniques which do well on analogy tasks, but do not leverage distributional information in corpus  \n",
    "1) Bengio (2003) uses a simple neural architecture,  Mikolov (2013a) uses skip gram and CBOW (single layer neural architectures), Levy (2014) - explicit word embeddingds based on PPMI metric  , vector log bilinear techniques (Mnih and Kavukcuoglo 2013) like vLBL and ivLBL)  \n",
    "2) CBOW and vLBL predict word given context, and skip gram and ivLBL predict context given word  \n",
    "3) These methods do not use cooccurence information , thus failing to take advantage of repetion in data (?)  \n",
    "4) Also, in these methods, computational cost scales with corpus size C, which makes computation much more expensive for large corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove  - Deriving the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Let X be word word matrix of co-occurence counts, X<sub>ij</sub> be count of word j in context i, X<sub>i</sub> be no of times any word appears in context of word i = $\\sum_k{X_{ik}}$ (rowsum of counts).\n",
    "Let$p_{j|i} = X_{ij}/X_{i}$ be the conditional probability of word j appearing in context i   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The goal of course is to get vector representations for every word from the co-occurence matrix X. Instead, of using traditional normalization and dimensionality reduction techniques such as length normalization of counts followed by LSA; In glove we define an objective function, based on some criteria below, and minimize it, this yields vectors for every word in the X matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Specifically, while defining the objective function, we use the observation  that instead of conditional probabilities themselves, the *ratio* of conditional probabilities in the presence of a third word k is better able to distinguish relevant and irrelevant words, and also discriminate betwee two relevant words.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Example - In figure pasted below (figure 1 of paper) Suppose we want to get representations of word i = ice, and j = steam.  \n",
    "a) For word k related to ice, but not to steam, $p_{k|ice}/p_{k|steam}$ will be very high  \n",
    "b) for word k related to steam but not ice, $p_{k|ice}/p_{k|steam}$ will be very small  \n",
    "c) and for word k which is equally related or unrelated to ice and steam, $p_{k|ice}/p_{k|steam}$ ~ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how_ratio_of_prob_is_better_than_raw_prob](glove_pic1.png \"Image Credit Figure 1 in paper\")       \n",
    "\n",
    "\n",
    "(Table 1 from paper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) In the table above, we see that $p_{k|ice}/p_{k|steam}$ is very high is k is the word 'solid', this is because solid in the context ice is much more frequent than solid in the context steam. Similarily $p_{k|ice}/p_{k|steam}$ is very low  if k is the word \"gas\" as counts of gas in the context of the word steam is much more frequent than count of gas in the context of ice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Using this observation, we this want to get an objective function F of word vectors to minimize such that\n",
    "$F(w_{i},w_{j},w_{\\tilde{k}}) = \\frac{p_{k|i}}{p_{k|j}}$  \n",
    "\n",
    "In other words, given word vectors $w_{i},w_{j},w_{\\tilde{k}}$ for words i, j and context vector $\\tilde{k}$ ; the objective function applied on word vectors should respect the (obtained from data) ratios of conditional probabilities; and the word vectors should be obtained in such a way that the function applied on them should respect the obtained from data) ratios of conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Since word vectors are expected to be linear, and RHS is a ratio of conditional probabilities, the linear equilavalent of which is a difference between word vectors, we change the equation above to $F(w_{i}-w_{j},w_{\\tilde{k}}) = \\frac{p_{k|i}}{p_{k|j}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) In addition, note that LHS arguments are vectors, RHS is a scalar. F could be some complex function which converts vectors to scalars, but this would obfuscate the linear structure we want. To make things simple, make the arguments of LHS a scalar by converting to dot product  $F( (w_{i}-w_{j})^{T}w_{\\tilde{k}}) = \\frac{p_{k|i}}{p_{k|j}}$    \n",
    "Note that the argument is now just the unnormalized cosine distance between w<sub>i</sub>-w<sub>j</sub> and w<sub>k</sub>\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) For word-word co-occurence matrix, distinction between word and context is arbitrary. This means that w can be replaced by $\\tilde{w}$; and X can be replaced by X<sup>T</sup>. This requires F to be a from the additive group of real numbers to the multiplicative group of positive real numbers. See [here](https://towardsdatascience.com/emnlp-what-is-glove-part-iii-c6090bed114) for more on homomorphism. This implies that  $F( (w_{i}-w_{j})^{T}w_{\\tilde{k}})  =.F( (w_{i})^{T}w_{\\tilde{k}})/F( (w_{j})^{T}w_{\\tilde{k}}) $  . This implies that F is the exponential functon. which satisfies this inequality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) From 7 and 9, This implies (numerators) that $F( (w_{i})^{T}w_{\\tilde{k}}) = p_{k|i} = X_{i\\tilde{k}}/X_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Taking ln on both sides, and knowing that F is the exponential function, $ (w_{i})^{T}w_{\\tilde{k}} = ln(p_{k|i}) = ln(X_{i\\tilde{k}}) - ln(X_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) X<sub>i</sub> is independent of k, so call it a word specific bias term b<sub>i</sub> which we can try to learn along with word vectors rather than obtaining from corpus . Why (?)\n",
    "\n",
    "$ (w_{i})^{T}w_{\\tilde{k}} = ln(X_{i\\tilde{k}}) - b_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Add a term $b_{\\tilde{k}}$ to restore symmetry to get \n",
    "\n",
    "$ (w_{i})^{T}w_{\\tilde{k}} + b_{i} + b_{k}  = ln(X_{i\\tilde{k}}) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) In equation above,  we want to learn all w<sub>i</sub>, b<sub>i</sub> and b<sub>k</sub>; $X_{ik}$ is known , obtained as counts from corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15) Here's a different way to get the equation above .\n",
    "Let's desire that learnt word and context vectors $w_{i}$ and $w_{\\tilde{k}}$   should satisfy the relation\n",
    "\n",
    "$(w_{i})^{T}w_{\\tilde{k}} = ln(P_{\\tilde{k}|i}) =  ln(X_{i\\tilde{k}}/X_{i}) = ln(X_{i\\tilde{k}}) - ln(X_{i}) $ \n",
    "\n",
    "\n",
    "Similarly, flipping i and $\\tilde{k}$,\n",
    "\n",
    "\n",
    "$(w_{\\tilde{k}})^{T}w_{i} = ln(P_{\\tilde{i}|k}) =  ln(X_{i\\tilde{k}}/X_{i}) = ln(X_{i\\tilde{k}}) - ln(X_{\\tilde{k}}) $ \n",
    "\n",
    "\n",
    "Adding both equations above, \n",
    "\n",
    "$(w_i)^{T}w_{\\tilde{k}}  = ln(X_{i\\tilde{k}}) - 0.5 ln(X_{i} - 0.5 ln X_{\\tilde{k}}$  =>\n",
    "\n",
    "$(w_{i})^{T}w_{\\tilde{k}} + b_{i} + b_{\\tilde{k}} = ln(X_{i\\tilde{k}}) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15) Now we go ahead , set cost function  \n",
    "$J =  \\sum_{i,k}((w_{i})^{T}w_{\\tilde{k}} + b_{i}  + b_{\\tilde{k}} - ln(X_{ik}))^{2}   $   \n",
    "\n",
    "we want to minimize this to find word vectors and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16) However, the equation above has problems - not defined when $X_{ik}$ = 0, also weights all cooccurences equally including rare cooccurences.  So add a weighting term f(X<sub>ij</sub>) to equation above - \n",
    "\n",
    "\n",
    "$J =  \\sum_{i,k}f(X_{ik})((w_{i})^{T}w_{\\tilde{k}} + b_{i}  + b_{\\tilde{k}} - ln(X_{ik}))^{2}   $   \n",
    "\n",
    "\n",
    "\n",
    "f(x) should satisfy the following properties. f(0) = 0, in fact f(x) should approach 0 as x approaches 0 fast enough that $lim_{x->0} {f(x)ln(x^{2})}$ -> 0 as x -> 0\n",
    "\n",
    "f(x) should be non-decreasing to awoid high weightage to rare cooccurences. \n",
    "\n",
    "f(x) should be relatively small for large values of x so that frequent cooccurences are not overweighted , should tend to plateau. \n",
    "\n",
    "\n",
    "\n",
    "The choice of f by the authors which satisfied the above 3 conditions were - $f(x) = (x/x_{max})^{\\alpha}$ if x < $x_{max}$, 1 otherwise, $\\alpha$ was chosen to be 3/4 heuristically\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![figure from paper on how f(x) varies with x](glove_pic2.png \"Image Credit Figure 1 in paper\")  \n",
    "\n",
    "\n",
    "(Figure 1 from paper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation to other models like W2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the authors try to cast w2vec objective functions in a form similar to GloVe to compare and contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) A skip gram or ivLBL mode tries to predict context given word i. Assuming a softmax probability, The probability of word j appearing in context of word i is\n",
    "\n",
    "$Q_{j|i} = \\frac{exp(w_i^{T}w_{j})}{\\sum_{k}{exp(w_i^{T}w_{k})}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We would like to minimize  $J = -\\sum_{i,j(i)} ln(Q_{j|i})$  \n",
    "where i spans across all words, j across all words in context for each i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 2 is equivalent to $J = -\\sum_{i,j}X_{ij}ln(Q_{j|i})$ grouping together terms with similar i and j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Note that $p_{ij} = X_{ij}/X_{i}$   =>  $J = -\\sum_{i}X_{i}\\sum_{j}{P_{j|i}ln(Q_{j|i})}$;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) 4 can be written as $J = \\sum_{i}X_{i}H(P_{i},Q_{i})$ where H is the cross entropy between distrobitions P and Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) 5 has some problems -  Q (model output) needs to be properly normalized, cross entropy H is just one of many distance measures between two distributions, which has the disadvantage that it gives too much weight rare counts for distributions with long tails. Also, computing softmax over the  entire vocabulary is computationally complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Therefore use a different distance metric $J = \\sum_{i,j}X_{i}(\\hat{P}_{j|i} - \\hat{Q}_{j|i})^{2}$    where $\\hat{P}_{j|i}$ and $\\hat{Q_{j|i}}$ are unnormalized, $\\hat{P}_{j|i} = X_{ij}$ and  $\\hat{Q}_{j|i} = exp(w_{i}^{T}w_{j})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) 7 has a problem where there is NO normalization, if counts are large, this can blow up. To avoid, minimize squared differece between logs\n",
    "\n",
    "Instead of $J = \\sum_{i,j}X_{i}(\\hat{P}_{j|i} - \\hat{Q}_{j|i})^{2}$ ,\n",
    "\n",
    "use \n",
    "\n",
    "$J = \\sum_{i,j}X_{i}(ln(\\hat{P}_{j|i}) - ln(\\hat{Q}_{j|i}))^{2}$ \n",
    "\n",
    "which means\n",
    "\n",
    "$J = \\sum_{i,j}X_{i}(w_{i}^{T}w_{j}-ln(X_{ij}))^{2}$   which is kind of analogous to the glove equation, absent bias terms which we can include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational cost of Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Repeating equation above, cost $J = \\sum_{i,j}f(X_{ij})((w_{i}^{T}w_{j}) + b_{i} + b_{j} - ln(X_{ij}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Computational cost depends on no of non-sparse elements in word-word matrix X which is of dimension V*V, V is vocab size. Therefore, in worst case of completely dense matrix, cost is O(V<sup>2</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Assume that $X_{ij}$ in presence of sparsity can be modeled as a power law function of frequency rank of word pair $r_{ij}$, ie $X_{ij} = \\frac{k}{r_{ij}^{\\alpha}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Number of words in corpus C |C| ~ $\\sum_{ij}X_{ij}$  =  $\\sum_{r=1}^{|X|}\\frac{k}{r_{ij}^{\\alpha}}$ = $kH_{|X|,\\alpha}$ where $H_{n,m} = \\sum_{1}^{n}\\frac{1}{r^{m}}$, a generalized harmonic number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Looking at the power law equation ie $X_{ij} = \\frac{k}{r_{ij}^{\\alpha}} $ , in the limiting case of $X_{ij}$ = 1 (atleast 1 count), $r_{ij} = k^{1/\\alpha}$. Therefore, if we rank $X_{ij}$, we get all ranks from 1 to $k^{1/\\alpha}$\n",
    "\n",
    "which means the no of non-zero terms in X |X| = $k^{1/\\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Therefore, substituting k = $|X|^{\\alpha}$ in equation |C| ~ kH(|X|,\\alpha), we get |C| ~ $|X|^{\\alpha}H(|X|,\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Using properties of generalized harmonic number,\n",
    "\n",
    "![generalized_harmonic_number](glove_pic3.png \"Image Credit Equation 20 in paper\")     where $\\zeta$ is the reimann zeta function\n",
    "\n",
    "\n",
    "(Equation 20 from the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) This gives  ![order_glove](glove_pic4.png \"Image Credit Equation 21 in paper\")     \n",
    "\n",
    "(Equation 21 from paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) This simplies for large X to |X| = O(|C|) if $\\alpha$ > 1 , and $O(|C|^{1/\\alpha})$ if $\\alpha$>1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. In practice, it is observed by the authors that $\\alpha$ = 1.25; which means that $|X| = O(|C|^{0.8})$; which is lesser than w2vec methods where $|X|$ = O(|C|)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Evaluated on word analogy tasks of Mikolov, word similarity tasks (Luong 2013), and on ConLL 2003 NER benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Word analogy - given 3 words a , b , c such that a:b::c:?, we find d as the word in corpus whose vector representation is closest to w_{b} + w_{c} - w{a}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) NER task - Used the ConLL benchmark , classifying each word into 4 entity types - organization, location, person, Misc . Model used was - ~437K discrete features from Stanford NER model + 50 dimensional vectors for each word in a 5 word context => trained a CRF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora used to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 5 data sets - 2010 wiki dump with  1 billion tokens, 2014 wiki dump with 1.6 billion tokens, gigaword 5 with 4.3 billion tokens, giga word 5 + wiki 2014 with ~ 6 billion tokens, and 42 billion tokens from crawling the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Each word tokenized and lower-cased using stanford tokenizer, vocab of 400K most frequent words built, matrix X of word-word cooccurence counts constructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Different window sizes for context, and whether to distinguish between left and right are tried. 1/d scaling function used within context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) NN Models trained using adaGrad, initial learning rate of 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![table 2 results](glove_pic5.png \"Table. 2 results\")  \n",
    "\n",
    "                Results on word analogy task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD is obtained from truncated matrix of top 10k frequent words, SVD-S - SVD of sqrt(X truncated), SVD-R - SVD of log(1+X truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![table 4 results](glove_pic6.png \"Table. 4 results\")  \n",
    "\n",
    "            Results on NER task (Table 4 from the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector length and context size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![table 2 results](glove_pic7.png \"Table. 2 results\")  \n",
    "\n",
    "            Performance on word analogy task (Figure 2 from the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
